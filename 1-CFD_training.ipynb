{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the 2L-SPC on CFD database\n",
    "https://arxiv.org/abs/2002.00892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  SPC_2L.DataTools import DataBase\n",
    "from SPC_2L.Network import LayerPC, Network\n",
    "from SPC_2L.Coding import ML_Lasso,ML_FISTA\n",
    "from SPC_2L.DataTools import DataBase, gaussian_kernel\n",
    "from SPC_2L.Monitor import Monitor\n",
    "from SPC_2L.Optimizers import mySGD, myAdam\n",
    "import torch.nn.functional as f\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import tensorboardX\n",
    "from SPC_2L.DataTools import LCN, whitening, z_score, mask, to_cuda, norm\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import pickle\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from SDPC.Monitor import Monitor\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools, parameters and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Not all batches have the same size, the last one will be dropped...\n"
     ]
    }
   ],
   "source": [
    "data_path = '../DataSet/CF_DB_training'\n",
    "\n",
    "LCN_params = {'kernel_size':11,'sigma':0.5, 'rgb':True} # Local contrast normalization parameters\n",
    "\n",
    "mask_params={'n': 10} # mask parameters\n",
    "\n",
    "whitening_params={'f_0':0.5,'n':2} # Whitening parameters\n",
    "\n",
    "Data_load_param = { 'batch_size': 10,\n",
    "                    'do_LCN': True,\n",
    "                    'LCN_params': LCN_params,\n",
    "                    'do_mask': True,\n",
    "                    'mask_params': mask_params,\n",
    "                    'do_whitening': True,\n",
    "                    'whitening_params': whitening_params,\n",
    "                    'do_z_score': True,\n",
    "                    'return_idx': False\n",
    "                  }\n",
    "\n",
    "Facedata = DataBase('from_ImageFolder', data_path, img_size=(100,124),reshaped_size=(120,171),**Data_load_param, shuffle=True)\n",
    "    \n",
    "## Setting parameters\n",
    "l_r = [1e-4,5e-3]#### dictionaries learning rate [Layer1, Layer2]\n",
    "l_rv = [1e-3,1e-3]#### normalizer learning rate [Layer1, Layer2]\n",
    "l = [0.5,1.8]#### Sparsity parameters [Layer1, Layer2]\n",
    "b=0#### Feedback strength parameter. b=0 --> Hila, b=1 --> 2L-SPC\n",
    "v_i=[6,6] #### Initial normalizer value [Layer1, Layer2]\n",
    "nb_epoch = 375 #### number of training epochs\n",
    "\n",
    "Use_tb = True #### Use to activate tensorboard monitoring\n",
    "save = True #### Use to run the entire simulation : TAKE HOURS. Use False to load previous simulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving the network (b=0 for Hi-La, b=1 for 2L-SPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK STRUCTURE : \n",
      " Input : (20, 3, 120, 171)\n",
      " Layer 1 : [20, 64, 38, 55]\n",
      " Layer 2 : [20, 128, 30, 47]\n"
     ]
    }
   ],
   "source": [
    "## Definition of the layers, network and sparse coding algorithm\n",
    "layer = [LayerPC((64, 3, 9, 9), stride=3, b=b, v=v_i[0], v_size=64 ,out_pad=0),\n",
    "        LayerPC((128, 64, 9, 9), stride=1, b=b, v=v_i[1], v_size=128 ,out_pad=0)]\n",
    "\n",
    "Net = Network(layer, input_size=(20, 3, 120, 171))\n",
    "Loss = ML_Lasso(Net, [l[0],l[1]])\n",
    "Pursuit = ML_FISTA(Net, Loss, max_iter=1000, th=5e-3, mode='eigen')\n",
    "\n",
    "## Optimizer initialization\n",
    "opt_dico = [None] * (Net.nb_layers + 1)\n",
    "for i in range(0, Net.nb_layers):\n",
    "    opt_dico[i] = mySGD([{'params': Net.layers[i].dico}], lr=l_r[i], momentum=0.9, normalize=True)\n",
    "\n",
    "opt_v = [myAdam([{'params': Net.layers[i].v}], lr=l_rv[i], normalize=False) \\\n",
    "         for i in range(Net.nb_layers)]\n",
    "\n",
    "L = [None] * (Net.nb_layers)\n",
    "L_v = [None] * (Net.nb_layers)\n",
    "reco = [None] * (Net.nb_layers)\n",
    "\n",
    "model_name = 'CFD_[{0},{1}]_b={2}'.format(l[0],l[1],b)\n",
    "path = 'Savings/CFD/' + model_name +'.pkl'\n",
    "\n",
    "if Use_tb : \n",
    "    nrows = [8,8,8,8,8,8,8]\n",
    "    writer = SummaryWriter('Savings/Log/' + model_name)\n",
    "    M = Monitor(Net, writer, n_row=nrows)\n",
    "\n",
    "k=0\n",
    "\n",
    "l2_loss = torch.zeros(2,nb_epoch*Facedata.size[0]*Facedata.size[1])\n",
    "l1_loss = torch.zeros(2,nb_epoch*Facedata.size[0]*Facedata.size[1])\n",
    "\n",
    "if save == True:\n",
    "    for e in range(nb_epoch):\n",
    "        for idx_batch, data in enumerate(Facedata.data):\n",
    "\n",
    "            batch = data[0].cuda()\n",
    "            gamma, it, Loss_G, delta = Pursuit.coding(batch)\n",
    "\n",
    "            for i in range(Net.nb_layers):\n",
    "                Net.layers[i].dico.requires_grad = True\n",
    "                L[i] = Loss.F(batch, gamma, i, do_feedback=False).div(batch.size()[0])  ## Unsupervised\n",
    "                L[i].backward()\n",
    "                Net.layers[i].dico.requires_grad = False\n",
    "                opt_dico[i].step()\n",
    "                opt_dico[i].zero_grad()\n",
    "                \n",
    "                l2_loss[i,k]= L[i].detach() \n",
    "                l1_loss[i,k] =  gamma[i].detach().sum().div(gamma[i].size(0))\n",
    "                \n",
    "            for i in range(Net.nb_layers):\n",
    "                Net.layers[i].v.requires_grad = True  # turn_on(i)\n",
    "                L_v[i] = Loss.F_v(batch, gamma, i).div(batch.size()[0])\n",
    "                L_v[i].backward()\n",
    "                Net.layers[i].v.requires_grad = False  # turn_off(i)\n",
    "                opt_v[i].step()  \n",
    "                opt_v[i].zero_grad()\n",
    "                \n",
    "            if Use_tb:\n",
    "                if (k%10) == 0:\n",
    "                    writer.add_scalar('FISTA_iterations', it, k)\n",
    "                    M.MonitorGamma(gamma, k, option=['NNZ', '%', 'Sum', 'V'])\n",
    "                    M.MonitorList(L, 'Loss_Dico', k)\n",
    "                    M.MonitorList(L_v, 'Loss_v', k)\n",
    "                    M.MonitorDicoBP(k)\n",
    "                    M.ComputeHisto(gamma)\n",
    "\n",
    "                if (k%100) == 0:\n",
    "                    reco = [None] * (Net.nb_layers)\n",
    "                    for i in range(Net.nb_layers-1,-1,-1):\n",
    "                        reco[i] = gamma[i]\n",
    "                        for j in range(i, -1, -1):\n",
    "                            reco[i] = Net.layers[j].backward(reco[i])\n",
    "                        reco_image = make_grid(reco[i],normalize=True,pad_value=1)\n",
    "                        writer.add_image('Reco/L{0}'.format(i),reco_image,k)\n",
    "\n",
    "            k += 1\n",
    "\n",
    "    output_exp = {'Net': Net,\n",
    "            'Loss': Loss,\n",
    "            'Pursuit': Pursuit,\n",
    "            'l2_loss': l2_loss,\n",
    "            'l1_loss': l1_loss    \n",
    "                 }\n",
    "    path = 'Savings/CFD/' + model_name +'.pkl'\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(output_exp, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :        \n",
    "    with open(path, 'rb') as file:\n",
    "        output_exp = pickle.load(file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
